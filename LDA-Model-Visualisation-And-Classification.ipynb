{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model visualisation\n",
    "#### A job worth doing\n",
    "\n",
    "\n",
    "This notebook explores a trained LDA topic model\n",
    "\n",
    "Ensure you have created a topic model using topic_model.py and that this notebook is in the root of the project\n",
    "\n",
    "1. Load and visualise a trained topic model\n",
    "2. Name each topic\n",
    "3. Classify job descriptions\n",
    "4. Further analysis using the results of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "# this import relies on the relative position of this notebook\n",
    "from lib.nlp_utils import NLTKProcessor\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and visualise a trained topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_MODEL_NAME = 'lda_model_20topics_10passes'\n",
    "LDA_MODEL_FILE_PATH = 'models/{}.model'.format(LDA_MODEL_NAME)\n",
    "# all of these methods reduce the high dimensional topics into 2 dimensional representations to be visualised\n",
    "#                    options: tsne, pcoa or mmds\n",
    "DIMENSION_REDUCTION_METHOD = 'tsne'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = LdaModel.load(LDA_MODEL_FILE_PATH)\n",
    "dictionary = Dictionary.load('models/dictionary.dict')\n",
    "doc_term_matrix = MmCorpus('models/corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates html visualisation of the topic model (sort_topics=Flase to preserve the topic index, so that a dictionary mapping can be made)\n",
    "# topic numbers in the visualisation start at 1, whereas in the gensim.models.LDAModel the index starts at 0\n",
    "html = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary, mds=DIMENSION_REDUCTION_METHOD, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the visualisation as a stand alone webpage\n",
    "pyLDAvis.save_html(html, 'visualisations/pyLDAvis.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Name each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hand created lookup between topic index from the pyLDAvis visualisation and the percieved category\n",
    "# the pyLDAvis index starts at 1, whereas the index within the topic model starts at 0\n",
    "TOPIC_LOOKUP = {\n",
    "    1: 'cleaning',\n",
    "    2: 'military',\n",
    "    3: 'care_takers',\n",
    "    4: 'construction',\n",
    "    5: 'other',\n",
    "    6: 'manufacturing',\n",
    "    7: 'other',\n",
    "    8: 'medical_and_insurance',\n",
    "    9: 'other',\n",
    "    10: 'project_and_business_management',\n",
    "    11: 'driving',\n",
    "    12: 'store_management',\n",
    "    13: 'warehouse', # pyshical effort / stooping\n",
    "    14: 'online_marketing', # sales, linkedin, youtube\n",
    "    15: 'software_development',\n",
    "    16: 'administration', # reports, compliance\n",
    "    17: 'healthcare_and_caring',\n",
    "    18: 'human_resource', # references to diversity, human resource?\n",
    "    19: 'accounting',\n",
    "    20: 'other' # references ajilon and randstad (recruitment agencies)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classify job descriptions\n",
    "### 3.1 Create functions and initialise a text processer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that gets the most probable classification of a job description using the topic model\n",
    "def classify_job_description(job_description, text_processer, dictionary, lda_model, topic_lookup=None, return_probability=False):\n",
    "    processed_job_description = text_processer.process(job_description)\n",
    "    bow_job_description = dictionary.doc2bow(processed_job_description.split())\n",
    "    classification_probabilities = lda_model[bow_job_description]\n",
    "    \n",
    "    # sort the classifications by the second element (probability of belonging to topic) then select the first element\n",
    "    classification = sorted(classification_probabilities, key=lambda x: x[1], reverse=True)[0]\n",
    "    if topic_lookup:\n",
    "        # +1 to align the indexes (different between pyLDAvis and gensim)\n",
    "        return topic_lookup[classification[0]+1] if not return_probability else classification[1]\n",
    "    else:\n",
    "        # if there is no topic lookup just return the index of the topic\n",
    "        return classification[0] if not return_probability else classification[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise the text processer that was originally used for the text processing\n",
    "text_processer = NLTKProcessor(stemmer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check that the pipeline is working using some example job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEST_JOB_DESCRIPTION = \"\"\"\n",
    "Performs administrative and office support activities for summer camp. Duties may include fielding telephone calls, \n",
    "receiving and directing visitors, data entry, creating and generating reports, sorting mail, and filing. Software skills, \n",
    "and strong communication skills are required. Camp Harkness serves up to 36 campers, youth and adults with special needs, \n",
    "at any given time, with a staff of approximately 15, including 12 counselors and a nurse. A current resume must be submitted \n",
    "as an indicator of interest in this position.\n",
    "\"\"\"\n",
    "\n",
    "classify_job_description(TEST_JOB_DESCRIPTION, \n",
    "                         text_processer, \n",
    "                         dictionary, \n",
    "                         lda_model, \n",
    "                         topic_lookup=TOPIC_LOOKUP, \n",
    "                         return_probability=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEESET_DATA_SCIENTIST_JOB_DESCRIPTION = \"\"\"\n",
    "Data Scientist required by an organisation in the North West who are investing heavily in their data analytics capabilities in 2018.\n",
    "If you want to work with Python and various sophisticated Machine Learning models on a daily basis and join a completely greenfield site with a scary amount of untapped data, read on.\n",
    "\n",
    "The Role\n",
    "You will also be joining a recently appointed Head of Data & Analytics who is keen to have someone in his team that he can work closely with, train up and develop in to a high calibre Data Scientist.\n",
    "As this is a fairly new area for the company, there is going to be a lot of initial grunt work to understand where they are at in relation to their data. You will help the new Head of Data Science find, scrape and collate data from various sources across the organisation to then make a start on making sense of it all.\n",
    "After the initial collation of the data, you will then play a key role in developing a strategy for the implementation of an advanced analytics platform across the entire product offering.\n",
    "\n",
    "Technical Stack:\n",
    "From an experience and technical perspective, the manager is happy to consider pure graduates from an MSc, PhD level or more experienced candidates as he has a track record of training people up in his previous role.\n",
    "Tech wise, he's most comfortable with Python in terms of programming so is likely to continue that in the new role. However, he's equally happy for people with strong SQL, Matlab or R skills to apply as he appreciates the similarities and how easy it can be to cross-train.\n",
    "From a Data Science perspective, we're looking for people with a real interest in most, if not all of the following areas:\n",
    "\n",
    "** Machine Learning \n",
    "** Statistics / Mathematics \n",
    "** Artificial Intelligence \n",
    "** Chatbots \n",
    "** Neural Networks \n",
    "** Python / R / SQL / Matlab\n",
    "\"\"\"\n",
    "\n",
    "classify_job_description(DEESET_DATA_SCIENTIST_JOB_DESCRIPTION, \n",
    "                         text_processer,\n",
    "                         dictionary, \n",
    "                         lda_model, \n",
    "                         topic_lookup=TOPIC_LOOKUP, \n",
    "                         return_probability=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Classify all the jobs in the monster.com job postings dataset using the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/final_data.csv'):\n",
    "    data_frame = pd.read_csv('data/cleansed_data.csv')\n",
    "    \n",
    "    # create a column and populate it with the classification\n",
    "    data_frame['classification'] = data_frame['job_description'].apply(classify_job_description, \n",
    "                                                                     args=(text_processer, dictionary, lda_model, TOPIC_LOOKUP))\n",
    "    \n",
    "    # create a column and populate it with the probability of the classification\n",
    "    data_frame['classification_probability'] = data_frame['job_description'].apply(classify_job_description,\n",
    "                                                                                   args=(text_processer, dictionary,\n",
    "                                                                                         lda_model, TOPIC_LOOKUP, True))\n",
    "    \n",
    "    data_frame.to_csv('data/final_data.csv', index=False)\n",
    "else:\n",
    "    data_frame = pd.read_csv('data/final_data.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use the new classifications for some further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up jupyter notebook for creating readable visualisations inline\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 12)\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Plot the distributions of salaries in each of the new classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "# remove outliers in the salary range (some hourly rates are actually yearly rates)\n",
    "quantile_data_frame = data_frame[data_frame['standardised_salary'] < data_frame['standardised_salary'].quantile(.99)]\n",
    "\n",
    "axis = quantile_data_frame.boxplot(column='standardised_salary', by='classification', ax=axis, rot=90, figsize=(40, 40))\n",
    "\n",
    "# edit to True to save figure\n",
    "if False:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualisations/Salary distribution by classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "1. improve salary parsing\n",
    "2. improve text processing, implement SpaCy for production level text preparation (to reduce noise in dataset)\n",
    "3. create a visualisation that shows the keywords for each category\n",
    "4. create a vector representations of all words in the dataset using word2vec\n",
    "5. engage more stakeholders in the topic labelling process and refine for business problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
